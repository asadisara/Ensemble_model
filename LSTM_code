import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from keras.models import Sequential
from keras.layers import LSTM, Dense
import shap
import matplotlib.pyplot as plt
import pkg_resources

# Get the version of the SHAP package
shap_version = pkg_resources.get_distribution("shap").version
print("SHAP version is:", shap_version)
print("Tensorflow version is:", tf.__version__)

# Load and preprocess the data
data = pd.read_csv('data.csv')  # Replace with the actual combined data file
data['Date'] = pd.to_datetime(data['Date'])
data.set_index('Date', inplace=True)

train_data = data['1990-01-01':'2005-12-31']
test_data = data['2006-01-01':'2020-09-28']

X_train = train_data[['SVR','FFNN','LSTM', 'SWAT']].values
y_train = train_data['ObservedQ(t)'].values
X_test = test_data[['SVR','FFNN','LSTM', 'SWAT']].values
y_test = test_data['ObservedQ(t)'].values

# Feature Scaling
scaler_X = StandardScaler()
X_train = scaler_X.fit_transform(X_train)
X_test = scaler_X.transform(X_test)

scaler_y = StandardScaler()
y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
y_test = scaler_y.transform(y_test.reshape(-1, 1)).flatten()

# Reshape data for LSTM (samples, time steps, features)
X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))

# Build the LSTM model
model = Sequential()
model.add(LSTM(4, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')

# Train the model
model.fit(X_train, y_train, epochs=100, batch_size=16, verbose=0)

# Save the trained model
model.save('lstm_model.h5')

# Predict using the trained model
y_pred_train = model.predict(X_train).flatten()
y_pred_test = model.predict(X_test).flatten()

# Inverse transform to get original scale
y_train = scaler_y.inverse_transform(y_train.reshape(-1, 1)).flatten()
y_test = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()
y_pred_train = scaler_y.inverse_transform(y_pred_train.reshape(-1, 1)).flatten()
y_pred_test = scaler_y.inverse_transform(y_pred_test.reshape(-1, 1)).flatten()

# Aggregate daily values to monthly values
monthly_observed_train = train_data['ObservedQ(t)'].resample('M').mean()
monthly_observed_test = test_data['ObservedQ(t)'].resample('M').mean()

monthly_simulated_train = pd.Series(y_pred_train, index=train_data.index).resample('M').mean()
monthly_simulated_test = pd.Series(y_pred_test, index=test_data.index).resample('M').mean()

# Calculate monthly metrics
def calculate_monthly_metrics (observed, simulated):
    metrics = {'NSE': 1 - (sum((observed - simulated)**2) / sum((observed - observed.mean())**2)), 'RMSE': np.sqrt(mean_squared_error(observed, simulated)), 'MAE': np.mean(np.abs(observed - simulated)), 'PBIAS': 100 * sum(simulated - observed) / sum(observed) } 
    return metrics

# Calculate metrics for train and test periods
metrics_train = calculate_monthly_metrics(monthly_observed_train, monthly_simulated_train)
metrics_test = calculate_monthly_metrics(monthly_observed_test, monthly_simulated_test)

# Save monthly values for observed and simulated flows
monthly_observed_train.to_excel('monthly_values_observed_train_lstm.xlsx', index=True)
monthly_simulated_train.to_excel('monthly_values_simulated_train_lstm.xlsx', index=True)
monthly_observed_test.to_excel('monthly_values_observed_test_lstm.xlsx', index=True)
monthly_simulated_test.to_excel('monthly_values_simulated_test_lstm.xlsx', index=True)

# Save monthly metrics
monthly_metrics_df = pd.DataFrame({'Train': metrics_train, 'Test': metrics_test})
monthly_metrics_df.to_excel('monthly_metrics_aggregated_lstm.xlsx')

# Additional code to calculate evaluation metrics
nse_train = 1 - (sum((y_train - y_pred_train)**2) / sum((y_train - y_train.mean())**2))
nse_test = 1 - (sum((y_test - y_pred_test)**2) / sum((y_test - y_test.mean())**2))

rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))
rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))

mae_train = np.mean(np.abs(y_train - y_pred_train))
mae_test = np.mean(np.abs(y_test - y_pred_test))

p_bias_train = 100 * sum(y_pred_train - y_train) / sum(y_train)
p_bias_test = 100 * sum(y_pred_test - y_test) / sum(y_test)

# Create a DataFrame to hold the evaluation metrics
evaluation_df = pd.DataFrame({
    'Metric': ['NSE', 'RMSE', 'MAE', 'PBIAS'],
    'Train': [nse_train, rmse_train, mae_train, p_bias_train],
    'Test': [nse_test, rmse_test, mae_test, p_bias_test]
})

# Save the evaluation metrics to a CSV file
evaluation_df.to_csv('evaluation_metrics_lstm.csv', index=False)

# Save simulated flow in CSV files
simulated_flow_train = pd.DataFrame({'Date': train_data.index, 'Simulated_Flow': y_pred_train.flatten()})
simulated_flow_test = pd.DataFrame({'Date': test_data.index, 'Simulated_Flow': y_pred_test.flatten()})

simulated_flow_train.to_csv('simulated_flow_train_lstm.csv', index=False)
simulated_flow_test.to_csv('simulated_flow_test_lstm.csv', index=False)

#Create an Explainer
explainer = shap.DeepExplainer(model, X_train)

#Generate SHAP values
shap_values = explainer.shap_values(X_train)


for i, shap_array in enumerate(shap_values):
    print(f"Shape of SHAP values for class {i}: {shap_array.shape}")

print(X_train.shape)

# Define your feature names
feature_names = ['SVR','FFNN','LSTM', 'SWAT+']

# Flatten the SHAP values and X_train data
shap_values_flattened = shap_values[0].reshape(shap_values[0].shape[0], -1)
X_train_flattened = X_train.reshape(X_train.shape[0], -1)

# Generate the SHAP summary plot for your regression model
shap.summary_plot(shap_values_flattened, X_train_flattened, feature_names=feature_names)


# Calculate the mean absolute SHAP values for each feature
mean_shap_values = np.abs(shap_values_flattened).mean(axis=0)
# Calculate the standard deviation of the SHAP values for each feature
std_shap_values = np.abs(shap_values_flattened).std(axis=0)

# Sort the features by mean importance
sorted_indices = np.argsort(mean_shap_values)[::-1]
sorted_feature_names = np.array(feature_names)[sorted_indices]
sorted_mean_shap_values = mean_shap_values[sorted_indices]
sorted_std_shap_values = std_shap_values[sorted_indices]

# Convert mean SHAP values to percentages
total = sorted_mean_shap_values.sum()
sorted_mean_shap_values_percent = 100 * sorted_mean_shap_values / total

# Create the bar plot with error bars
plt.figure(figsize=(10, 6))
bars = plt.barh(range(len(sorted_feature_names)), sorted_mean_shap_values_percent, color=(0.561, 0.667, 0.863), capsize=3)
plt.errorbar(sorted_mean_shap_values_percent, range(len(sorted_feature_names)), xerr=sorted_std_shap_values, fmt='none', ecolor='black', elinewidth=1, capthick=1, capsize=5)
plt.xlabel('Mean Absolute SHAP Value (Importance %)', fontsize=16)  # Adjust font size
# Add the importance percentage next to each bar, shifted to the right for clarity, and in dark red color
for bar in bars:
    plt.text(bar.get_width() + max(sorted_std_shap_values) + 0.5, bar.get_y() + bar.get_height()/2, f'{bar.get_width():.2f}%', va='center', color=(1,0,0.25), fontsize=14)

# Adjust the starting position of the horizontal axis
plt.xlim(left=-2, right=max(sorted_mean_shap_values_percent) + max(sorted_std_shap_values) + 4)

plt.yticks(range(len(sorted_feature_names)), sorted_feature_names, fontsize=16)
plt.xticks(fontsize=14) 
plt.xlabel('Mean Absolute SHAP Value (Importance %)', fontsize=16)
plt.title('Global Feature Importance_station 3030', fontsize=20, loc='left')
plt.gca().invert_yaxis()  # To display the highest importance at the top
plt.tight_layout()  # Adjust the layout to fit all elements
plt.savefig('GlobalFeatureImp_station3030.png', dpi=300)  # Save the first figure with high resolution
plt.show()

# Plot SHAP dependency plots with trendline and R² for each feature
for i, feature in enumerate(feature_names):
    plt.figure(figsize=(8, 4))
    plt.yticks(fontsize=14)
    plt.xticks(fontsize=14)
    plt.xlabel(f'{feature} Value', fontsize=16)  # Adjust font size
    plt.ylabel('SHAP Value', fontsize=16)  # Adjust font size
    plt.title(f'SHAP Dependence Plot for {feature}_station 3030', fontsize=20, loc='left')
    plt.scatter(X_train_flattened[:, i], shap_values_flattened[:, i], c='blue', alpha=0.5)

    # Fit a trendline to the data
    z = np.polyfit(X_train_flattened[:, i], shap_values_flattened[:, i], 1)
    p = np.poly1d(z)

    # Plot the trendline
    plt.plot(X_train_flattened[:, i], p(X_train_flattened[:, i]), 'r--')

    # Calculate and print R² value
    r2 = r2_score(shap_values_flattened[:, i], p(X_train_flattened[:, i]))
    plt.text(0.05, 0.92, f'R²: {r2:.3f}', transform=plt.gca().transAxes, fontsize=16)

    plt.xlabel(f'{feature} Value')
    plt.ylabel('SHAP Value')
    plt.savefig(f'feature_{i}_{feature}_high_res_station3030.png', dpi=300)
    plt.show()
